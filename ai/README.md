# TCP Agent AI Platform

This directory contains the complete AI/ML implementation for the TCP Agent platform, including machine learning models, ClickHouse integration, and training pipelines for transfer optimization.

## ğŸ—ï¸ Directory Structure

```
ai/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config.json                  # Configuration file (auto-generated)
â”œâ”€â”€ clickhouse_client.py         # ClickHouse integration client
â”œâ”€â”€ train_with_clickhouse.py     # End-to-end training pipeline
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ AI_IMPLEMENTATION_STATUS.md     # Current implementation status
â”‚   â”œâ”€â”€ CLICKHOUSE_SETUP.md             # ClickHouse setup guide
â”‚   â””â”€â”€ CLICKHOUSE_INTEGRATION_PLAN.md  # Integration plan
â”‚
â”œâ”€â”€ docker/                      # Docker configurations
â”‚   â””â”€â”€ docker-compose.clickhouse.yml   # ClickHouse Docker setup
â”‚
â”œâ”€â”€ scripts/                     # Setup and utility scripts
â”‚   â”œâ”€â”€ setup_clickhouse.sh      # ClickHouse setup script
â”‚   â””â”€â”€ setup_environment.py     # Python environment setup
â”‚
â”œâ”€â”€ features/                    # Feature engineering
â”‚   â””â”€â”€ engineering.py           # Feature extraction pipeline
â”‚
â”œâ”€â”€ models/                      # ML models
â”‚   â”œâ”€â”€ performance_predictor.py # XGBoost performance prediction
â”‚   â”œâ”€â”€ anomaly_detector.py      # LSTM anomaly detection
â”‚   â”œâ”€â”€ resource_allocator.py    # PPO resource allocation
â”‚   â””â”€â”€ scheduling_optimizer.py  # DQN scheduling optimization
â”‚
â”œâ”€â”€ data/                        # Data directories (auto-created)
â”‚   â”œâ”€â”€ cache/                   # Cached data
â”‚   â””â”€â”€ temp/                    # Temporary files
â”‚
â””â”€â”€ logs/                        # Log files (auto-created)
    â””â”€â”€ ai_platform.log          # Main log file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Set up Python environment and dependencies
cd ai/
python3 scripts/setup_environment.py

# Or manually:
pip install -r requirements.txt
```

### 2. ClickHouse Setup

```bash
# Start ClickHouse using Docker
./scripts/setup_clickhouse.sh

# Or manually:
cd docker/
docker-compose -f docker-compose.clickhouse.yml up -d
```

### 3. Run Training Pipeline

```bash
# Run complete training pipeline
python3 train_with_clickhouse.py
```

## ğŸ”§ Configuration

The `config.json` file contains all configuration settings:

```json
{
  "clickhouse": {
    "host": "localhost",
    "port": 8123,
    "database": "tcp_optimization"
  },
  "models": {
    "performance_predictor": {
      "model_dir": "models/performance",
      "retrain_interval_hours": 24
    }
  }
}
```

## ğŸ“Š Models

### 1. Performance Predictor
- **Type**: XGBoost Regressor
- **Purpose**: Predict transfer throughput and completion time
- **Features**: Network metrics, system metrics, transfer settings
- **Output**: Predicted throughput (Mbps), completion time (minutes)

### 2. Anomaly Detector
- **Type**: Hybrid (Isolation Forest + LSTM Autoencoder)
- **Purpose**: Detect abnormal transfer performance
- **Features**: Time series of network and system metrics
- **Output**: Anomaly score, anomaly type classification

### 3. Resource Allocator
- **Type**: PPO (Proximal Policy Optimization)
- **Purpose**: Optimize bandwidth and compute resource allocation
- **Features**: Agent demands, system capacity, priorities
- **Output**: Resource allocation per agent

### 4. Scheduling Optimizer
- **Type**: DQN (Deep Q-Network)
- **Purpose**: Optimize transfer scheduling based on network conditions
- **Features**: Network conditions, queue state, historical performance
- **Output**: Optimal scheduling decisions

## ğŸ—„ï¸ ClickHouse Integration

### Database Schema

**tcp_telemetry** table stores all transfer telemetry:
- Network metrics (bandwidth, latency, packet loss)
- Transfer metrics (throughput, duration, chunk size)
- System metrics (CPU, memory, disk I/O)
- Environmental data (time, day of week)
- Optimization results

**ml_model_performance** table tracks model performance:
- Model metrics (accuracy, MAE, RMSE, RÂ²)
- Training statistics
- Inference performance

### Data Pipeline

1. **Collection**: TCP agents send telemetry to ClickHouse
2. **Processing**: Feature engineering pipeline extracts ML features
3. **Training**: Models train on historical data
4. **Inference**: Real-time predictions for optimization
5. **Feedback**: Results stored back to ClickHouse

## ğŸ§ª Testing

### Run Basic Tests
```bash
# Test environment setup
python3 scripts/setup_environment.py --test

# Test ClickHouse connectivity
./scripts/setup_clickhouse.sh test

# Test training pipeline with sample data
python3 train_with_clickhouse.py --sample-data
```

### Validate Models
```bash
# Check model performance
python3 -c "
from models.performance_predictor import TransferPerformancePredictor
predictor = TransferPerformancePredictor()
predictor.load_model('latest')
print('Model loaded successfully')
"
```

## ğŸ“ˆ Monitoring

### ClickHouse Web UI
- URL: http://localhost:8123/play
- Query telemetry data directly
- Monitor system performance

### Logs
```bash
# View AI platform logs
tail -f logs/ai_platform.log

# View ClickHouse logs
docker-compose -f docker/docker-compose.clickhouse.yml logs -f
```

## ğŸ”„ Training Pipeline

The complete training pipeline (`train_with_clickhouse.py`) includes:

1. **Data Loading**: Fetch telemetry from ClickHouse
2. **Feature Engineering**: Extract and transform features
3. **Model Training**: Train all ML models
4. **Validation**: Cross-validation and performance metrics
5. **Model Saving**: Persist trained models
6. **Performance Tracking**: Record metrics in ClickHouse

## ğŸ› ï¸ Development

### Adding New Models

1. Create model class in `models/` directory
2. Implement training and prediction methods
3. Add to training pipeline
4. Update configuration
5. Add tests

### Feature Engineering

1. Add feature extraction methods to `features/engineering.py`
2. Update feature validation
3. Test with sample data
4. Document new features

### ClickHouse Schema Changes

1. Update schema in `clickhouse_client.py`
2. Create migration script
3. Update training pipeline
4. Test with existing data

## ğŸš¨ Troubleshooting

### Common Issues

**ClickHouse Connection Failed**
```bash
# Check if ClickHouse is running
docker ps | grep clickhouse

# Restart ClickHouse
./scripts/setup_clickhouse.sh restart
```

**Import Errors**
```bash
# Reinstall dependencies
python3 scripts/setup_environment.py --force-reinstall
```

**Training Failures**
```bash
# Check data availability
python3 -c "
import asyncio
from clickhouse_client import create_clickhouse_client
async def check():
    client = await create_clickhouse_client()
    df = await client.get_training_data(hours=24)
    print(f'Available records: {len(df)}')
asyncio.run(check())
"
```

### Performance Optimization

**ClickHouse Performance**
- Increase memory allocation in Docker Compose
- Optimize table partitioning
- Use materialized views for frequent queries

**Model Training**
- Use GPU acceleration (CUDA)
- Implement data sampling for large datasets
- Use early stopping to prevent overfitting

## ğŸ“š Documentation

- [AI Implementation Status](docs/AI_IMPLEMENTATION_STATUS.md) - Current progress and next steps
- [ClickHouse Setup Guide](docs/CLICKHOUSE_SETUP.md) - Detailed setup instructions
- [ClickHouse Integration Plan](docs/CLICKHOUSE_INTEGRATION_PLAN.md) - Architecture and design decisions

## ğŸ¤ Contributing

1. Follow the existing code structure
2. Add comprehensive tests
3. Update documentation
4. Use type hints and docstrings
5. Test with sample data before committing

## ğŸ“„ License

This AI platform is part of the TCP Agent project. See the main project README for license information. 